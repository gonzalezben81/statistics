# Bivariate Regression

```{r setup,echo=FALSE,message=FALSE}
library(mathjaxr)
library(latexpdf)
library(ggplot2)
library(dplyr)
library(DiagrammeR)

```


## Bivariate Regression Terminology {-}

+ **Regression** – is the process of estimating a best-fitting line that summarizes the relationship between a predictor variable (Independent Variable) and a criterion variable (Dependent Variable).

+ **Regression Analysis** – researchers fit a regression line to a sample of data, estimate the parameters of the regression equation (i.e., the constant and regression coefficient), and use the resulting equation to predict scores on a criterion variable. 

+ **Bivariate** – means that the analyses discussed include just 2 variables, a predictor variable (the X variable), and a criterion variable (the Y variable).

+ **Linear** – refers to the fact, when the Y scores are plotted against the X scores, it should be possible to fit a best-fitting straight line through the center of the scores, as opposed to a best-fitting curved line. 

## Regression: Generic Analysis Model {-}

---

<center>
```{r,echo=FALSE}
DiagrammeR::grViz("
  digraph graph2 {
  
  graph [layout = dot, rankdir = LR]
  
  # node definitions with substituted label text
  node [shape = square]
  Predictor [label = 'Predictor (IV):\n - Multi Value \n - Interval or Ratio Scale']
  Criterion [label = 'Criterion (DV):\n - Multi Value \n - Interval or Ratio Scale']
  
  Predictor -> Criterion
  }
  
  [1]: Interval
  [2]: Ratio

  ",
  height = 200)
```
</center>

---

<!-- <center> -->
<!-- ### Bivariate Regression -->
<!-- </center> -->


## Assumptions of Bivariate Regression {-}

+ **Linearity** – fit a best-fitting straight line through the scatter plot.

+ **Independence** – each observation included in the sample should be drawn independently from the population of interest. Researchers should not have taken repeated measures on the same variable from the same participant. 

+ **Homogeneity of Variance (Homoscedasticity)** – the variance of the Y scores should remain fairly constant at all values of X.

+ **Normality** – residuals of prediction should be normally distributed.
Bivariate Normality – for any specific score on one of the variables, scores on the other variable should follow a normal distribution.


## Bivariate Regression Formula

Here we have the formula for the bivariate regression equation.
The regression equation takes the following form:
$$\Large
Regression\;Equation:\;\;\;\hat{y} = a + \beta(X)
$$
$$
\hat{y}\;\;–\;the\;predicted\;score\;on\;the\;criterion\;variable
$$
$$
a\;–\;the\;constant\;or\;the\;intercept\;of\;the\;regression\;equation.
$$
$$
\beta\;–\;the\;unstandardized\;regression\;coefficient.
\newline
\newline
\newline
Represents\;the\;amount\;of\;change\;in\;Y\;that\;is \;associated\;with\;a\;one-unit\;change\;in\;X\;\\when\;both\;variables\;are\;in\;raw\;score\;form.\;Also\;known\;as\;the\;regression\;weight\;or\;slope.
$$


<!-- $$\Large -->
<!-- \hat{y} = a + \beta(X) -->
<!-- $$ -->

## Scatterplot of the data set

Here we can plot our data to get a good look at the shape of the data set.

+ **Scatterplot** – a graph that illustrates the nature of the relationship between two quantitative variables. 
+ **X Axis** – Predictor Variable - hp
+ **Y Axis** – Criterion Variable - mpg

We can utilize the following plot function to create a basic scatterplot in R.
<br>
```{r,eval=FALSE}
attach(mtcars)
with(data = mtcars,plot(x = hp,y = mpg,col="black",pch=19,main="Mtcars"))
```
<br>

```{r,echo=FALSE}
attach(mtcars)
with(data = mtcars,plot(x = hp,y = mpg,col="black",pch=19,main="Mtcars"))
```


```{r,echo=FALSE}
mpg_prediction <- lm(formula = mpg~hp,data = mtcars)

```

## Calculating the Residual

Here we compute the residual by taking the actual y value and subtract the predicted y value. The residual for each observation is the difference between the predicted values of y and the actual values of y. Calculating the residual helps us to see if we have overpredicted or underpredicted for $\hat{y}$.

$$\Large
Residual = actual\;y\;value - predicted\;y\;value
$$

$$\Large
r_{1} = y_{i} - \hat{y_{i}}
$$

```{r,echo=FALSE}

print('Predicted y Values')
print(mpg_prediction$fitted.values)

print('Actual y Values')
print(mtcars$mpg)

print('Manually Calculated Residuals')
table<- as.data.frame(mtcars$mpg-mpg_prediction$fitted.values)
print(table)
# d1 <- table[1:16,]
# d2 <- table[17:32,]
# knitr::kable(
#   list(d1, d2),
#   caption = 'Residuals of Prediction.',
#   booktabs = TRUE, valign = 't'
# )
# knitr::kable(table)

print('Residual Values')
residual_mpg_values<- mpg_prediction$model$mpg-mpg_prediction$fitted.values
print(residual_mpg_values)


# sum(mpg_prediction$residuals^2)
# 
# residual_sum <- sum(mpg_prediction$residuals^2)
# sum_mean <- sum((mtcars$mpg-mean(mtcars$mpg)^2))

```

## Calculate the mean of the Y Values

Here we find the mean of our criterion (y) value of mpg.

$$\Large
\bar{y} = \frac{\sum{y}}{n}
$$

We can utilize the ***mean*** function to the calculate the mean of mpg (miles per gallon).
```{r}
mean(mtcars$mpg)
```

## Coefficient of Determination or ${R^2}$

**Coefficient of Determination** – indicates the percent of **variance in the criterion** variable **that is accounted for by the predictor** variable. 

---

$$
Coefficient\;of\;Determination:\;\;R^2 =\;1-\; \frac{sum\;squared\;regression\;(SSR)}{sum\;squares\;total\;(SST)}
$$

$$
=1- \frac{\sum(y_{i}\;-\;\hat{y_{i}})^2}{\sum(y_{i}\;-\;\overline{y})^2}
\\
\\
y_{i} = actual\;y\;values
\\
\hat{y_i} = predicted\;y\;values
\\
\overline{y} = mean\;of\;y
\\
\sum\;or\;sigma = sum
$$

---


### Calculate the numerator of the formula - Sum Squared Regression (SSR)

$$
\sum(y_{i}\;-\;\hat{y_{i}})^2
$$
```{r}
top_of_formula <- sum(mpg_prediction$residuals^2)
print(top_of_formula)
```
### Calculate the denominator of the formula - Sum Squares Total (SST)

---

$$
\sum(y_{i}\;-\;\overline{y})^2
$$

---

```{r}
bottom_of_formula <- sum((mtcars$mpg-mean(mtcars$mpg))^2)
print(bottom_of_formula)
```
---

$$
R^2 = 1- \frac{447.6743}{1126.047}
$$
$$
R^2 = 1- 0.3975627
$$
$$
R^2 = 0.6024373
\\
R^2 = 0.6024
$$

---

## Calculate the Adjusted-R Squared $Adj.R^2\;or\;R^2_{adj}$

---

$$
Adj.R^2\;or\;R^2_{adj} = 1 - (1-R^2)\;\cdot\;(n-1)/(n-p-1)
\\
Adj.R^2\;or\;R^2_{adj} = 1 - (1-0.6024373)\;\cdot\;(32-1)/(32-1-1)
\\
Adj.R^2\;or\;R^2_{adj} = 0.5891852
\\
R^2 = coefficient\;of\;determination
\\
n = number\;of\;observations
\\
p=number\;of\;predictors
$$

---

---

```{r}
adj.r.squared = 1 - (1 - 0.6024373) * ((32 - 1)/(32-1-1))
print(adj.r.squared)
```
## Utilize the lm function in R to automate our work

Here we can utilize the ***lm*** function in R to perform our bivariate regression (simple linear regression). This will allow us to save the model to a variable and then utilize the **\$** (dollar sign) operator in R. The **\$** (dollar sign) operator allows us to pull out things we need such as the residuals and fitted values that are returned from the summary function.

### Print the residuals of the model

```{r}
mpg_hp_model <- lm(mpg ~ hp, mtcars)
print(mpg_hp_model$residuals)
```

### Print the coefficients of the model

```{r}
mpg_hp_model <- lm(mpg ~ hp, mtcars)
print(mpg_hp_model$coefficients)
```

#### Print the fitted values of the model
```{r}
mpg_hp_model <- lm(mpg ~ hp, mtcars)
print(mpg_hp_model$fitted.values)
```

### Putting it altogether

Here we can print out the summary of the model utilizing the ***summary*** function in R; `summary(mpg_hp_model)`. We can also plot the predicted y values with the actual y values. Then we can draw a line between each of the predicted values and the actual values.This helps us visualize the amount of variation that is present between the predicted vs the actual values of y.

```{r,eval=FALSE}
mpg_hp_model = lm(mpg ~ hp, mtcars)
print(summary(mpg_hp_model))
```

## Plot our residuals and a best fitting line

Here we can utilize the `ggplot2` package to plot our model. We can also plot the residuals along with a best fitting line.

```{r,eval=FALSE}
mtcars %>% ggplot(aes(hp,mpg))+
  geom_point()+
  geom_smooth(method = "lm")+
  geom_linerange(aes(ymax = mpg, ymin = mpg-resid),color="red")
```

```{r,echo=FALSE}
mpg_hp_model = lm(mpg ~ hp, mtcars)

print(summary(mpg_hp_model))

resid = mpg_hp_model$residuals

mtcars %>% ggplot(aes(hp,mpg))+
  geom_point()+
  geom_smooth(method = "lm")+
  geom_linerange(aes(ymax = mpg, ymin = mpg-resid),color="darkred")
```

```{r,echo=FALSE}
mpg_prediction <- lm(formula = mpg~hp,data = mtcars)

```

Now we can take a predictor value (X) and plug it in. We then are able to predict where our criterion value (Y) will be.

$$
\hat{y}\;=\;30.09886\;+\;-0.06823(X)
$$

$$
\hat{y}\;=\;30.09886\;+\;-0.06823(335)
$$
$$
\hat{y}\;=\;30.09886\;+\;-22.85705
$$
$$
\hat{y} = 7.24
$$

---

## Squaring the correlation ${r}$ to find the coefficient of determination ${R^2}$

According to Hatcher (2013) we can simply square the correlation provided we are looking at only one predictor variable and one dependent variable. When we square the correlation coefficient this will give us the coefficient of determination. 

### Correlation coefficient of hp and mpg
```{r}
cor(x = mtcars$hp,mtcars$mpg)

```

${r} = -0.7761684$

### Squaring the correlation coefficient

Here we can square the correlation coefficient ${r}$ and it will give us the coefficient of determination or ${R^2}$


```{r}
cor(x = mtcars$hp,mtcars$mpg)^2
```

${R^2} = 0.6024373$

Here we can see we get the same value for the coefficient of determination ${R^2}$ by squaring the correlation as if we had utilized the ***lm*** function. However the ***lm*** function has advantages as it provides us with our p-value, F-statistic, and the intercept and the unstandardized regression coefficient.

